{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Environment Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/01 16:42:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    ".getOrCreate())\n",
    "\n",
    "path = \"/Users/c800409/NUIT_Workshop/data/parquet/\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read csv files and write to parquet\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transaction Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/01 13:26:42 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/06/01 13:26:42 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "22/06/01 13:26:42 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "22/06/01 13:26:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "22/06/01 13:26:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transaction_data = spark.read.option(\"header\",\"true\").csv(\"/Users/c800409/NUIT_Workshop/data/csv/transaction_data.csv\")\n",
    "transaction_data.repartition(10).write.mode(\"OVERWRITE\") \\\n",
    "        .parquet(f\"{path}transaction_data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Campaign Desc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "campaign_desc = spark.read.option(\"header\",\"true\").csv(\"/Users/c800409/NUIT_Workshop/data/csv/campaign_desc.csv\")\n",
    "campaign_desc.repartition(1).write.mode(\"OVERWRITE\") \\\n",
    "        .parquet(f\"{path}campaign_desc\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Campaign Table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "campaign_table = spark.read.option(\"header\",\"true\").csv(\"/Users/c800409/NUIT_Workshop/data/csv/campaign_table.csv\")\n",
    "campaign_table.repartition(1).write.mode(\"OVERWRITE\") \\\n",
    "        .parquet(f\"{path}campaign_table\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Causal Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/01 16:42:37 ERROR Executor: Exception in task 3.0 in stage 1.0 (TID 4) 12]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:349)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:228)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda$2810/0x00000008012a8840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2485/0x0000000801129040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2450/0x00000008010eec40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 1.0 (TID 4),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:349)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:228)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda$2810/0x00000008012a8840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2485/0x0000000801129040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2450/0x00000008010eec40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 WARN TaskSetManager: Lost task 3.0 in stage 1.0 (TID 4) (localhost executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:349)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:228)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda$2810/0x00000008012a8840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2485/0x0000000801129040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2450/0x00000008010eec40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "22/06/01 16:42:37 ERROR TaskSetManager: Task 3 in stage 1.0 failed 1 times; aborting job\n",
      "22/06/01 16:42:37 ERROR FileFormatWriter: Aborting job f486b09c-4727-4a1c-8698-e73c73ddfd6b.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 4) (localhost executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:349)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:228)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda$2810/0x00000008012a8840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2485/0x0000000801129040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2450/0x00000008010eec40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:349)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:228)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda$2810/0x00000008012a8840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2485/0x0000000801129040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2450/0x00000008010eec40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_797e7d99-cdfe-44bc-8b4d-c5d5589aa452\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_797e7d99-cdfe-44bc-8b4d-c5d5589aa452 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_797e7d99-cdfe-44bc-8b4d-c5d5589aa452\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_15032508-4592-4d96-9ae5-654e78d5810d\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_15032508-4592-4d96-9ae5-654e78d5810d (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_15032508-4592-4d96-9ae5-654e78d5810d\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_89bd321b-3eba-4d73-9628-223be99bc20f\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_89bd321b-3eba-4d73-9628-223be99bc20f (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_89bd321b-3eba-4d73-9628-223be99bc20f\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_7e107673-de4d-4fd5-80f7-75d9458af6bf\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_7e107673-de4d-4fd5-80f7-75d9458af6bf (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_7e107673-de4d-4fd5-80f7-75d9458af6bf\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0c/temp_shuffle_c25c64cc-3786-403e-80f2-c0f1bfce2389\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0c/temp_shuffle_c25c64cc-3786-403e-80f2-c0f1bfce2389 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0c/temp_shuffle_c25c64cc-3786-403e-80f2-c0f1bfce2389\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_cd0e20b3-103d-4c25-916a-cfaa25dc365e\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_cd0e20b3-103d-4c25-916a-cfaa25dc365e (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_cd0e20b3-103d-4c25-916a-cfaa25dc365e\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_60de8785-17ab-46d9-ae3b-95381a86e11d\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_60de8785-17ab-46d9-ae3b-95381a86e11d (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_60de8785-17ab-46d9-ae3b-95381a86e11d\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_25421f18-9358-414d-90f7-9296a7217407\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_25421f18-9358-414d-90f7-9296a7217407 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_25421f18-9358-414d-90f7-9296a7217407\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_1d52784e-2616-4064-968b-dae7cafba810\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_1d52784e-2616-4064-968b-dae7cafba810 (Invalid argument)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_1d52784e-2616-4064-968b-dae7cafba810\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0c/temp_shuffle_4bf07c2c-ad99-49e7-af76-65bbd731f499\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0c/temp_shuffle_4bf07c2c-ad99-49e7-af76-65bbd731f499 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0c/temp_shuffle_4bf07c2c-ad99-49e7-af76-65bbd731f499\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3e/temp_shuffle_c7cf919f-0431-47af-9e6f-1898eeb9ed5d\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3e/temp_shuffle_c7cf919f-0431-47af-9e6f-1898eeb9ed5d (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3e/temp_shuffle_c7cf919f-0431-47af-9e6f-1898eeb9ed5d\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_8402e1c2-f01b-423e-8cc7-334e0470db54\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_8402e1c2-f01b-423e-8cc7-334e0470db54 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_8402e1c2-f01b-423e-8cc7-334e0470db54\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3c/temp_shuffle_823600e9-db7d-477c-999c-1bbe04ae2a85\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_f778292d-6c04-48ca-a256-50909ab42bb6\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_f778292d-6c04-48ca-a256-50909ab42bb6 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_f778292d-6c04-48ca-a256-50909ab42bb6\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_d5eabc24-9918-49e3-90de-5a7ec63dce64\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_d5eabc24-9918-49e3-90de-5a7ec63dce64 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0d/temp_shuffle_d5eabc24-9918-49e3-90de-5a7ec63dce64\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_1b78e9fa-5088-4d1f-8107-6dd8196fa583\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_1b78e9fa-5088-4d1f-8107-6dd8196fa583 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_1b78e9fa-5088-4d1f-8107-6dd8196fa583\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_a22410a3-d038-4615-94cd-5fe7e6869b3f\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_a22410a3-d038-4615-94cd-5fe7e6869b3f (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_a22410a3-d038-4615-94cd-5fe7e6869b3f\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_7d5d1677-b716-48bc-a77c-ef599f7374fd\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_7d5d1677-b716-48bc-a77c-ef599f7374fd (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_7d5d1677-b716-48bc-a77c-ef599f7374fd\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_69204f94-692c-467a-a8db-3ef0028e9ef8\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_69204f94-692c-467a-a8db-3ef0028e9ef8 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_69204f94-692c-467a-a8db-3ef0028e9ef8\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_70a0b42c-b896-42cc-b960-912db510342d\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_70a0b42c-b896-42cc-b960-912db510342d (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_70a0b42c-b896-42cc-b960-912db510342d\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/03/temp_shuffle_fe5fd42c-eea7-4c2d-a7e8-17039375ffa9\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/03/temp_shuffle_fe5fd42c-eea7-4c2d-a7e8-17039375ffa9 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/03/temp_shuffle_fe5fd42c-eea7-4c2d-a7e8-17039375ffa9\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3c/temp_shuffle_9fa6f63c-2676-4d20-a3ff-bc31366039c9\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3c/temp_shuffle_9fa6f63c-2676-4d20-a3ff-bc31366039c9 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3c/temp_shuffle_9fa6f63c-2676-4d20-a3ff-bc31366039c9\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_d27df129-3fbc-4c83-ae29-6c9015a75fda\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_d27df129-3fbc-4c83-ae29-6c9015a75fda (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_d27df129-3fbc-4c83-ae29-6c9015a75fda\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/32/temp_shuffle_a72e5170-49e9-4c01-806f-c4e719d781fc\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/32/temp_shuffle_a72e5170-49e9-4c01-806f-c4e719d781fc (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/32/temp_shuffle_a72e5170-49e9-4c01-806f-c4e719d781fc\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/03/temp_shuffle_fdb14b3a-4444-4a7f-a571-a45563d853a4\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/03/temp_shuffle_fdb14b3a-4444-4a7f-a571-a45563d853a4 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/03/temp_shuffle_fdb14b3a-4444-4a7f-a571-a45563d853a4\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_16b3eca3-a2c3-49df-9593-515531a13238\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_16b3eca3-a2c3-49df-9593-515531a13238 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/35/temp_shuffle_16b3eca3-a2c3-49df-9593-515531a13238\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_2fda8ac4-f80d-4490-b324-92e014924142\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_2fda8ac4-f80d-4490-b324-92e014924142 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_2fda8ac4-f80d-4490-b324-92e014924142\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_75399f79-c389-450c-bb80-81ecfaec73f6\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_75399f79-c389-450c-bb80-81ecfaec73f6 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_75399f79-c389-450c-bb80-81ecfaec73f6\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_4c73c41c-dc42-400b-8da5-bbccf4f6a689\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_4c73c41c-dc42-400b-8da5-bbccf4f6a689 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/04/temp_shuffle_4c73c41c-dc42-400b-8da5-bbccf4f6a689\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_a5e1c96c-8bde-4bda-a301-43a1acd141ba\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_a5e1c96c-8bde-4bda-a301-43a1acd141ba (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/3b/temp_shuffle_a5e1c96c-8bde-4bda-a301-43a1acd141ba\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_bac8ae71-750f-46a5-ac15-ffac3a155261\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_bac8ae71-750f-46a5-ac15-ffac3a155261 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0b/temp_shuffle_bac8ae71-750f-46a5-ac15-ffac3a155261\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0c/temp_shuffle_b86bc89c-de73-4cf8-977d-7e638da1a814\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0c/temp_shuffle_b86bc89c-de73-4cf8-977d-7e638da1a814 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/0c/temp_shuffle_b86bc89c-de73-4cf8-977d-7e638da1a814\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/05/temp_shuffle_000f9638-910a-40b3-96ac-7af45315df94\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/05/temp_shuffle_000f9638-910a-40b3-96ac-7af45315df94 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/05/temp_shuffle_000f9638-910a-40b3-96ac-7af45315df94\n",
      "22/06/01 16:42:37 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/03/temp_shuffle_7d829290-0f43-4fa1-9741-3cafc21da883\n",
      "java.io.FileNotFoundException: /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/03/temp_shuffle_7d829290-0f43-4fa1-9741-3cafc21da883 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 16:42:37 ERROR BypassMergeSortShuffleWriter: Error while deleting file /private/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/blockmgr-65075be4-6dcb-4566-920e-35514e49d67d/03/temp_shuffle_7d829290-0f43-4fa1-9741-3cafc21da883\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/49/z9y8tsn16pv_g58zyq39hxhmj7x7s3/T/ipykernel_34131/2797530677.py\", line 3, in <cell line: 3>\n",
      "    causal_data.repartition(50).write.mode(\"OVERWRITE\") \\\n",
      "  File \"/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/pyspark/sql/readwriter.py\", line 885, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/c800409/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.808s][warning][gc,alloc] Executor task launch worker for task 3.0 in stage 1.0 (TID 4): Retried waiting for GCLocker too often allocating 131074 words\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m causal_data \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/c800409/NUIT_Workshop/data/csv/causal_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mcausal_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrepartition\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mOVERWRITE\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/Users/c800409/NUIT_Workshop/data/parquet/causal_data\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/pyspark/sql/readwriter.py:885\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m    884\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n\u001B[0;32m--> 885\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m py4j\u001B[38;5;241m.\u001B[39mprotocol\u001B[38;5;241m.\u001B[39mPy4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31m<class 'str'>\u001B[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2004\u001B[0m, in \u001B[0;36mInteractiveShell.showtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2001\u001B[0m     traceback\u001B[38;5;241m.\u001B[39mprint_exc()\n\u001B[1;32m   2002\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2004\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_showtraceback\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2005\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall_pdb:\n\u001B[1;32m   2006\u001B[0m     \u001B[38;5;66;03m# drop into debugger\u001B[39;00m\n\u001B[1;32m   2007\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdebugger(force\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py:538\u001B[0m, in \u001B[0;36mZMQInteractiveShell._showtraceback\u001B[0;34m(self, etype, evalue, stb)\u001B[0m\n\u001B[1;32m    532\u001B[0m sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mflush()\n\u001B[1;32m    533\u001B[0m sys\u001B[38;5;241m.\u001B[39mstderr\u001B[38;5;241m.\u001B[39mflush()\n\u001B[1;32m    535\u001B[0m exc_content \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    536\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraceback\u001B[39m\u001B[38;5;124m\"\u001B[39m: stb,\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mename\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(etype\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m),\n\u001B[0;32m--> 538\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevalue\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    539\u001B[0m }\n\u001B[1;32m    541\u001B[0m dh \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplayhook\n\u001B[1;32m    542\u001B[0m \u001B[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001B[39;00m\n\u001B[1;32m    543\u001B[0m \u001B[38;5;66;03m# to pick up\u001B[39;00m\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/protocol.py:471\u001B[0m, in \u001B[0;36mPy4JJavaError.__str__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__str__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    470\u001B[0m     gateway_client \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_exception\u001B[38;5;241m.\u001B[39m_gateway_client\n\u001B[0;32m--> 471\u001B[0m     answer \u001B[38;5;241m=\u001B[39m \u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexception_cmd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    472\u001B[0m     return_value \u001B[38;5;241m=\u001B[39m get_return_value(answer, gateway_client, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    473\u001B[0m     \u001B[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001B[39;00m\n\u001B[1;32m    474\u001B[0m     \u001B[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001B[39;00m\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001B[39;00m\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1016\u001B[0m     \u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/clientserver.py:281\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 281\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_new_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/clientserver.py:288\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    285\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[1;32m    287\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 288\u001B[0m     \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect_to_java_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[1;32m    290\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/clientserver.py:402\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[1;32m    401\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[0;32m--> 402\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_address\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_port\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    404\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "causal_data = spark.read.option(\"header\",\"true\").csv(\"/Users/c800409/NUIT_Workshop/data/csv/causal_data.csv\")\n",
    "\n",
    "causal_data.repartition(50).write.mode(\"OVERWRITE\") \\\n",
    "        .parquet(\"/Users/c800409/NUIT_Workshop/data/parquet/causal_data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Coupon"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "coupon = spark.read.option(\"header\",\"true\").csv(\"/Users/c800409/NUIT_Workshop/data/csv/coupon.csv\")\n",
    "coupon.repartition(2).write.mode(\"OVERWRITE\") \\\n",
    "        .parquet(f\"{path}coupon\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### coupon_redempt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "coupon_redempt = spark.read.option(\"header\",\"true\").csv(\"/Users/c800409/NUIT_Workshop/data/csv/coupon_redempt.csv\")\n",
    "coupon_redempt.repartition(2).write.mode(\"OVERWRITE\") \\\n",
    "        .parquet(f\"{path}coupon_redempt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### hh_demographic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "hh_demographic = spark.read.option(\"header\",\"true\").csv(\"/Users/c800409/NUIT_Workshop/data/csv/hh_demographic.csv\")\n",
    "hh_demographic.repartition(1).write.mode(\"OVERWRITE\") \\\n",
    "        .parquet(f\"{path}hh_demographic\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Product"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "Input \u001B[0;32mIn [61]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m product \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/c800409/NUIT_Workshop/data/csv/product.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m product\u001B[38;5;241m.\u001B[39mrepartition(\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOVERWRITE\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mproduct\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/pyspark/sql/session.py:755\u001B[0m, in \u001B[0;36mSparkSession.read\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    743\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    744\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    745\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;124;03m    Returns a :class:`DataFrameReader` that can be used to read data\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;124;03m    in as a :class:`DataFrame`.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    753\u001B[0m \u001B[38;5;124;03m    :class:`DataFrameReader`\u001B[39;00m\n\u001B[1;32m    754\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 755\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameReader\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrapped\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/pyspark/sql/readwriter.py:53\u001B[0m, in \u001B[0;36mDataFrameReader.__init__\u001B[0;34m(self, spark)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, spark):\n\u001B[0;32m---> 53\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jreader \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ssql_ctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark \u001B[38;5;241m=\u001B[39m spark\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/java_gateway.py:1320\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1313\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1322\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1016\u001B[0m     \u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/clientserver.py:281\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 281\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_new_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/clientserver.py:288\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    285\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[1;32m    287\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 288\u001B[0m     \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect_to_java_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[1;32m    290\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/NUIT_Workshop/venv/lib/python3.8/site-packages/py4j/clientserver.py:402\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[1;32m    401\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[0;32m--> 402\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_address\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_port\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    404\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "product = spark.read.option(\"header\",\"true\").csv(\"/Users/c800409/NUIT_Workshop/data/csv/product.csv\")\n",
    "product.repartition(2).write.mode(\"OVERWRITE\") \\\n",
    "        .parquet(f\"{path}product\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}