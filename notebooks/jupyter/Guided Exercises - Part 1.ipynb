{"cells":[{"cell_type":"markdown","source":["# Guided Exercises\n\n## Goals:\n\n- Read data\n- Write data\n- View and explore data \n- Peform basic calculations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8887e802-7389-49bd-b7e5-1464fcc0bb63"}}},{"cell_type":"markdown","source":["## Read, Write, and View the Data\n\nWe can use dbutils to view the data available for this workshop."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0220b06-d934-4f0c-8ba9-d7eac4aedc35"}}},{"cell_type":"code","source":["data_path = \"abfss://attributes@sa8451ccnthwstext.dfs.core.windows.net/\"\n\ndisplay(dbutils.fs.ls(data_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f63c31f-9e74-4a96-9c3b-de9f72eee5fd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's take a look inside the transaction data directory. \n1. How many files are there? \n2.  What are the file extensions? \n3.  What is the advantage of splitting the data into multiple small files vs. one large file, like a single csv?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92c4434f-fd57-4f89-a490-3f574dca9774"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(f\"{data_path}/transaction_data\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c93fbfaf-c7ca-46ae-ae59-cc6770060c76"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Parquet Files\n\n- Binary file that contains meta data about its contents\n   - Embedded schema in the file\n- Columnar data format\n  - <img src ='https://www.dremio.com/wp-content/uploads/2022/04/chart-1024x412.png' width=700>\n  - Not human readable\n  - Reads much more efficiently\n- Why is parquet a better choice than CSV?\n  - Parquet’s *Write Once Read Many* paradigm\n    - Slow to write but incredibly fast to read, especially when you’re only accessing a subset of the total columns\n  - CSV is row-based and uses eager evaluation\n    - Reads and parses entire dataset before performing transformations\n    - More costly and less efficient for bigger jobs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32cf58f7-ef47-456f-bbdc-154492e2c3e2"}}},{"cell_type":"markdown","source":["<img src ='https://media-exp1.licdn.com/dms/image/C5612AQHDkeQFY4ujZQ/article-inline_image-shrink_1500_2232/0/1568305562015?e=1665619200&v=beta&t=M9UPgyesA1vKiteJjTXKg7OR8DpRlhOd9yQCL-sTuuw' width=800>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a50ce50-2ab9-4a3b-86ed-d02f577f2286"}}},{"cell_type":"markdown","source":["More Resources:\n- https://www.linkedin.com/pulse/spark-file-format-showdown-csv-vs-json-parquet-garren-staubli/\n- https://www.dremio.com/resources/guides/intro-apache-parquet/\n- https://umbertogriffo.gitbook.io/apache-spark-best-practices-and-tuning/storage/use-the-best-data-format\n- https://luminousmen.com/post/big-data-file-formats"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8df39b7b-3c23-4e8a-b759-4390a7441e62"}}},{"cell_type":"markdown","source":["### Partitioning\n\n- **Partitioning** is the splitting of data into smaller, more manageable chunks\n- Data can be partitioned both:\n  - On disk (where the files are saved in the file system)\n  - In memory (data loaded into Spark itself as a DataFrame)\n- Saved data is automatically partitioned into multiple files by Spark\n  - Allows for transformations to be executed in parallel\n- Data can be partitioned based on the value in a certain column\n  - This column is called the **Partition Key**\n  - Loading data that is partitioned by a column that you’re going to filter on can make loading data *much* faster"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e0c5cf9-1e36-4a80-940a-b91b9a538522"}}},{"cell_type":"markdown","source":["### Reading Parquet Files\n\n`read.parquet()` reads in the data from a parquet file. The result of loading a parquet file is a Spark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dad89a6-0d90-477e-a4bc-091d99401c4f"}}},{"cell_type":"code","source":["transaction_data = spark.read.parquet(f\"{data_path}/transaction_data/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15e417a6-fb13-42c6-b603-620c4d698f6d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Writing Parquet Files\n\n`write.parquet()` writes out the data in parquet file form"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24dba120-d100-4b52-aa38-d322bb6aed92"}}},{"cell_type":"code","source":["results_container =  \"abfss://results@sa8451ccnthwstext.dfs.core.windows.net\"\n\n## REPLACE WIH YOUR EMAIL ###\nuser = \"your.email@gmail.com\"\n#############################\n\noutput_path = f\"{results_container}/{user}/transaction_data_output\"\nprint(output_path)\n\n### UNCOMMENT BELOW WHEN READY TO WRITE ##########\n# transaction_data.write.mode(\"overwrite\").parquet(output_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1eea4210-d361-44ad-8375-d191f299e43f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(dbutils.fs.ls(output_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bc3ca62-9848-43a8-84da-e3c15ce83644"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["`partitionBy(\"partitionKey\")` partitions the data by the specified partition key (i.e. a column in the dataframe) when writing\n\n**Why do this?** Allows for quicker reads when restricting to certain parition key values\n\n**What's the drawback?** You'll spend more time writing now than before (because of the data shuffling). But it'll save you so much time on reads!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b21512c4-fada-44dd-8650-6919e52be02c"}}},{"cell_type":"code","source":["output_partitioned_path = f\"{results_container}/{user}/transaction_data_output_by_week\"\nprint(output_partitioned_path)\n\ntransaction_data.write.partitionBy(\"WEEK_NO\").mode(\"overwrite\").parquet(output_partitioned_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"PartitionBy","showTitle":false,"inputWidgets":{},"nuid":"2243727b-e373-4196-9d8c-83928f545428"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's look at those partitions!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15e7b2ce-f44b-4acf-ad7b-827cac766d76"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(output_partitioned_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cd82fde-88b2-4b1e-a1cb-342c4e493302"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Print Schema\n\n`printSchema()` displays the schema of the dataframe. `printSchema()` is a PySpark function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b87ca18-cf8b-4717-b17c-469e59b2849b"}}},{"cell_type":"code","source":["transaction_data.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Show Transaction Schema","showTitle":false,"inputWidgets":{},"nuid":"576f5754-9aff-4619-abe3-8d3430950227"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Display/Show\n\nUse `display()` or `show()` to display a preview of the records in the dataframe. Keep in mind that `display()` is a Databricks specific function whereas `show()` is a PySpark specific function that can be used anywhere (i.e. in the terminal)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88e6c7d9-ae72-4052-b2bb-09eaa671f318"}}},{"cell_type":"code","source":["transaction_data.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Transactions at a glance","showTitle":false,"inputWidgets":{},"nuid":"ae00ca1c-c7c7-403f-8816-2aab927accd1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["transaction_data.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5649801-c63f-4b29-9b0b-6dbdf4531c8c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Explore the Data\n\nExamples of stuff we can cover:\n\n- How many records are there?\n- What are some metrics of the data?\n- How many distinct records are there?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf7bf5cc-11cb-4f6f-8f39-0b24b12fe00f"}}},{"cell_type":"markdown","source":["### Describe\n\n`describe()` displays some metrics about the dataframe. `describe()` is a PySpark function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"580c12d9-ca78-4f16-a671-b6c08f4ac833"}}},{"cell_type":"code","source":["transaction_data.describe().display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Describe","showTitle":false,"inputWidgets":{},"nuid":"257ed8a5-0fb5-4e22-b312-1f85d1a46175"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Count\n\n`count()` returns the number of records in the  dataframe. `count()` is a PySpark function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf7f0530-ef74-4559-9a79-aa3049015418"}}},{"cell_type":"code","source":["transaction_data.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55ed693f-0462-45b1-81b6-6c49132d9999"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Select\n\n`select()` narrows down the dataframe to only the specified columns. `select()` is a PySpark function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62ce9b25-8cf9-4037-b9dc-ebc937caae84"}}},{"cell_type":"code","source":["transaction_data.select(\"household_key\", \"BASKET_ID\", \"PRODUCT_ID\").display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"670419d6-7697-4645-94b4-094d748cd9c2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Distinct\n\n`distinct()` removes any duplicate records from the dataframe and returns the dataframe with only unique records. You **cannot** choose a subset of columns to perform the `distinct()` on."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02673663-51b7-45d9-b9ee-429af593bf87"}}},{"cell_type":"code","source":["transaction_data.distinct().display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e1a6ac9-4e1e-4a20-aa7a-38d6e8463df3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["transaction_data.select(\"household_key\", \"BASKET_ID\").distinct().display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e8e3860-bc43-4093-b212-d003372368dd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["transaction_data.select(\"household_key\", \"BASKET_ID\").distinct().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d840f71-893e-4e72-ba04-d30ca85009f7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Drop Duplicates\n\n`dropDuplicates()` removes any duplicate records from the dataframe and returns the dataframe with only unique records. You **can** choose a subset of columns to perform the `dropDuplicates()` on while still returning all columns in the dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8282a70a-47e0-48e2-85e4-f64516a2f420"}}},{"cell_type":"code","source":["transaction_data.dropDuplicates().display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9b5f915-5af8-4416-958c-caf605159bfe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["transaction_data.dropDuplicates([\"household_key\", \"BASKET_ID\"]).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd0a99cb-7352-4750-9661-a666575c68b3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["transaction_data.dropDuplicates([\"household_key\", \"BASKET_ID\"]).count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d4c79bd-64ee-489a-877b-7266557a4cf5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Questions\n\n1.  How many records are in the coupon data?\n2.  What's the min/max of each of the columns in the coupon data?\n3.  How many unique coupon records are there?\n4.  How many unique products are in the coupon data?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21cfc9a9-a781-4b82-9d29-21e91072c94b"}}},{"cell_type":"markdown","source":["ANSWER TO QUESTION 1 - How many records are in the coupon data?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4888911b-5b64-4421-8ab3-2de828c85ff5"}}},{"cell_type":"code","source":["# QUESTION 1: Read in the coupon data\ncoupon = spark.read.parquet(f\"{data_path}/coupon/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f490bc57-8d2f-4be8-9e84-a69490f3c6fc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# QUESTION 1: Perform a count to get the number of records in the coupon data\ncoupon.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6eea72dc-6797-4f67-8d74-3f6bdb59633a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["ANSWER TO QUESTION 2 - What's the min/max of each of the columns in the coupon data?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a583a071-31eb-4a78-a6c2-eead5ede3974"}}},{"cell_type":"code","source":["# QUESTION 2: Perform a describe to easily find the min/max of each of the columns in the coupon data\ncoupon.describe().display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee8134f1-649a-446a-8ef1-5e7d93c4069f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["ANSWER TO QUESTION 3 - How many unique coupon records are there?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"751142e6-7b21-46ee-9c46-43e36c232779"}}},{"cell_type":"code","source":["# QUESTION 3: Method 1 - Perform a distinct to find the number of unique coupon records\ncoupon.distinct().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be7a400e-c454-4c83-9031-322975ec55e2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# QUESTION 3: Method 2 - Perform a dropDuplicates without specifying a subset to find the number of unique coupon records\ncoupon.dropDuplicates().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e5aae82-0a38-4995-badb-edd1a7944b51"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["ANSWER TO QUESTION 4 - How many unique products are in the coupon data?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87c84110-11f3-43c5-bde4-ea2729b92e11"}}},{"cell_type":"code","source":["# QUESTION 4: Method 1 - Perform a select to get only the PRODUCT_ID column, then perform a distinct to get the number of unique products\ncoupon.select(\"PRODUCT_ID\").distinct().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e071dbe-2ca4-47eb-8fc2-9875b6b01ea2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# QUESTION 4: Method 2 - Perform a dropDuplicates on PRODUCT_ID subset to get the number of unique products\ncoupon.dropDuplicates([\"PRODUCT_ID\"]).count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fb26ec1-67b1-453e-9f0e-a2823db8847e"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Guided Exercises - Part 1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2630057572783364}},"nbformat":4,"nbformat_minor":0}
